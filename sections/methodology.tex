\section{Methodology}
\label{sec:methodology}

This section describes our implementation of arithmetic coding and the probability models evaluated in this study.

\subsection{Arithmetic Coding Framework}

\subsubsection{Encoder}
% Describe encoding algorithm
\subsubsection{Encoder Mechanics and State Management}
Our arithmetic encoder maintains an interval $[L, H)$ initialized to $[0, 1)$. For each symbol $s$ with cumulative probability range $[P_{\text{low}}(s), P_{\text{high}}(s))$, the interval is updated as:
\begin{align}
    \text{range} &= H - L \\
    H' &= L + \text{range} \times P_{\text{high}}(s) \\
    L' &= L + \text{range} \times P_{\text{low}}(s)
\end{align}

\paragraph{Precision Management via Renormalization}
As the interval narrows, it eventually requires more precision than standard floating-point types provide. To maintain numerical stability, we implement a renormalization process that outputs bits incrementally. This is governed by three scaling conditions (E1, E2, E3), formalized as follows:

\begin{itemize}
    \item \textbf{E1 Condition (High-Bit Converged to 0)}: If $H < 0.5$, the entire interval lies in the lower half. We output a \texttt{0}, usually followed by any pending E3 bits as \texttt{1}s. The interval is rescaled: $L \leftarrow 2L, H \leftarrow 2H$.
    
    \item \textbf{E2 Condition (High-Bit Converged to 1)}: If $L \geq 0.5$, the interval lies in the upper half. We output a \texttt{1}, followed by pending E3 bits as \texttt{0}s. Rescale: $L \leftarrow 2(L - 0.5), H \leftarrow 2(H - 0.5)$.
    
    \item \textbf{E3 Condition (Straddle Case caused by Underflow risk)}: If $L \geq 0.25$ and $H < 0.75$, the interval is "stuck" around the midpoint. We cannot commit a bit yet, so we increment a \texttt{pending\_bits} counter and zoom in on the middle: $L \leftarrow 2(L - 0.25), H \leftarrow 2(H - 0.25)$.
\end{itemize}

\paragraph{Encoding Trace Example}
To illustrate this, Table~\ref{tab:trace} shows the state progression for encoding the sequence "BAB" with a static model $P(A)=0.6, P(B)=0.2, P(EOF)=0.2$.

\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|l|l|}
\hline
\textbf{Step} & \textbf{Symbol} & \textbf{Interval $[L, H)$} & \textbf{Action} & \textbf{Output} \\ \hline
Init & - & $[0.0, 1.0)$ & - & - \\ \hline
1 & B & $[0.6, 0.8)$ & - & - \\ \hline
1a & - & $[0.2, 0.6)$ & E2 Scaling & 1 \\ \hline
1b & - & $[0.4, 1.2)$ & E3 Scaling & (pending++) \\ \hline
2 & A & $[0.4, 0.88)$ & - & - \\ \hline
\end{tabular}
\caption{Trace of interval updates and scaling operations.}
\label{tab:trace}
\end{table}

\subsubsection{Decoder}
% Describe decoding algorithm
The decoder maintains a value $v$ representing the encoded number and the same interval $[L, H)$. For each symbol, it determines which probability range contains $v$ and updates the interval accordingly.

\subsection{Probability Models}

\subsubsection{Markov Models}
% Implementation details of order-1, 2, 3 Markov models
We implement adaptive Markov models of orders 1, 2, and 3.
\begin{itemize}
    \item \textbf{Order-1}: Context is the previous symbol, requires $|\Sigma|^2$ counters. Implemented with a full 2D array.
    \item \textbf{Order-2}: Context is the previous 2 symbols, requires $|\Sigma|^3$ counters. Implemented with a full 3D array.
    \item \textbf{Order-3}: Context is the previous 3 symbols. Since a full table ($|\Sigma|^4$) is prohibitively large (~32GB), we implement a \textbf{sparse model} using a hash map (`containers.Map`). Only observed contexts are stored, significantly reducing memory usage.
\end{itemize}

% Escape mechanism and smoothing
To handle unseen contexts, we use an escape mechanism with Laplace smoothing: each context starts with count 1 for all symbols.

\subsubsection{Finite State Machine Models}
% Describe FSM model design
Our FSM model is specifically designed to efficiently encode \textbf{run-length sequences}. It extends the Order-1 Markov model by adding a binary "Run State" to the context:
\begin{itemize}
    \item \textbf{Context}: $[s_{t-1}, \text{IsRun}]$
    \item \textbf{IsRun}: True if $s_{t-1} == s_{t-2}$, else False.
\end{itemize}
This allows the model to maintain separate probability distributions for "normal" transitions versus "run" transitions, enabling it to quickly adapt to repeated symbols.

\subsubsection{Neural Network Models}
% Describe RNN/LSTM architecture
We implement a custom \textbf{Long Short-Term Memory (LSTM)} network from scratch in MATLAB to avoid the overhead of the Deep Learning Toolbox for symbol-by-symbol updates. The LSTM architecture is chosen to better capture long-range dependencies compared to simple RNNs.
\begin{itemize}
    \item \textbf{Architecture}: Standard LSTM cell with Input, Forget, and Output gates, plus a Cell state.
    \begin{itemize}
        \item \textbf{Forget Gate ($f_t$)}: Controls what information is discarded from the cell state.
        \item \textbf{Input Gate ($i_t$)}: Controls what new information is stored in the cell state.
        \item \textbf{Output Gate ($o_t$)}: Controls what parts of the cell state are output to the hidden state.
    \end{itemize}
    \item \textbf{Output Layer}: A Softmax layer converts the hidden state into a probability distribution over the 256 possible symbols.
    \item \textbf{Training}: Online Stochastic Gradient Descent (SGD) with Backpropagation Through Time (BPTT) truncated to 1 step.
    \item \textbf{Initialization}: Deterministic weight initialization using sinusoidal functions to ensure identical states for encoder and decoder.
\end{itemize}
The model updates its weights after every symbol, allowing it to adapt to the data stream in real-time.

\subsubsection{Context Mixing Models}
To leverage the complementary strengths of different models, we implemented a \textbf{Context Mixing} strategy. This meta-model combines the probability estimates of multiple sub-models (e.g., Markov and FSM) to produce a final prediction.
\begin{itemize}
    \item \textbf{Weighted Averaging}: The probability of a symbol $s$ is calculated as $P(s) = \sum_{i} w_i P_i(s)$, where $w_i$ is the weight of model $i$.
    \item \textbf{Adaptive Weights}: Weights are dynamically updated based on prediction accuracy. Models that assign higher probability to the actual observed symbol have their weights increased, allowing the system to shift focus to the most effective model for the current local context.
\end{itemize}

\subsection{Implementation Details}
% MATLAB implementation specifics
All models are implemented in MATLAB using object-oriented programming (handle classes) to maintain state across symbol processing. Key implementation features include:
\begin{itemize}
    \item \textbf{Arithmetic Coding}: Implemented using double-precision floating-point arithmetic with renormalization (E1, E2, E3 scaling) to maintain numerical stability.
    \item \textbf{Data Structures}: Full multi-dimensional arrays are used for context tables in 1st and 2nd order Markov models, as the alphabet size ($|\Sigma|=256$) allows for direct indexing without excessive memory overhead for these orders.
    \item \textbf{Modularity}: The system uses a polymorphic design where the arithmetic coder accepts any model object that implements the required \texttt{get\_range} and \texttt{update} methods.
\end{itemize}

The code is structured to allow easy swapping of probability models while maintaining the same arithmetic coding engine.
