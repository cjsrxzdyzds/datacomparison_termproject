\section{Conclusion}
\label{sec:conclusion}

This paper presented a comprehensive comparative analysis of probability models for arithmetic coding, evaluating Markov models, Finite State Machine models, and neural network models across diverse datasets.

\subsection{Key Findings}
Our experimental results demonstrate:
\begin{enumerate}
    \item \textbf{Neural models dominate compression}: The LSTM model achieved the best compression ratios on both text and binary data, outperforming traditional Markov models. This suggests that neural networks' ability to model continuous state spaces is superior for capturing complex dependencies.
    \item \textbf{Context dilution limits high-order Markov}: The 3rd Order Markov model performed worse than expected due to data sparsity in our test files, highlighting a key limitation of discrete context tables.
    \item \textbf{Precision matters for binary data}: We identified that standard floating-point arithmetic coding can struggle with the high-entropy nature of binary data, requiring higher precision (52-bit) to avoid decoding errors.
    \item \textbf{Trade-offs are unavoidable}: The best compressing model (LSTM) was also the slowest by a large margin, confirming the fundamental trade-off between compression efficiency and computational complexity.
\end{enumerate}

\subsection{Practical Contributions}
This work provides:
\begin{itemize}
    \item A unified framework for comparing diverse probability models
    \item Quantitative guidelines for model selection
    \item Open-source MATLAB implementation for educational use
    \item Empirical evidence of compression-complexity trade-offs
\end{itemize}

\subsection{Broader Impact}
Understanding the trade-offs between probability models for arithmetic coding has implications for:
\begin{itemize}
    \item Designing efficient compression systems for resource-constrained devices
    \item Developing adaptive compressors that select models based on data characteristics
    \item Advancing neural compression techniques by identifying their strengths and limitations
\end{itemize}

\subsection{Project Execution Roadmap}
The development of this project followed a structured four-phase roadmap, ensuring a systematic exploration of probability models:

\subsubsection{Phase 1: Foundation (Week 1)}
We established the core arithmetic coding engine and implemented baseline Markov models (1st and 2nd Order). Verification focused on correctness using small test strings and unit tests.

\subsubsection{Phase 2: Advanced Models (Week 2)}
We expanded the model suite to include:
\begin{itemize}
    \item \textbf{3rd Order Markov}: Implemented with sparse hashmaps to handle the $256^3$ state space.
    \item \textbf{FSM}: Designed for run-length encoding to handle binary data.
    \item \textbf{LSTM}: A custom neural network implementation for sequence prediction.
\end{itemize}

\subsubsection{Phase 3: Experiments \& Analysis (Week 3)}
We conducted extensive benchmarking on the Canterbury Corpus, measuring compression ratios and execution time. New analytical tools were developed, including:
\begin{itemize}
    \item \textbf{Context Mixing}: A meta-model to combine predictions.
    \item \textbf{Correlation Analysis}: A study of model orthogonality.
\end{itemize}

\subsubsection{Phase 4: Finalization (Week 4)}
The final phase focused on synthesizing results into this report, generating comparative visualizations, and documenting the trade-offs between model complexity and performance.

\subsection{Final Remarks}
While arithmetic coding with sophisticated probability models can approach theoretical entropy limits, practical considerations of speed and memory often favor simpler models. The optimal choice depends on the specific application context, and our comparative analysis provides the empirical foundation for making informed decisions.

Future work should explore hybrid approaches that combine the strengths of different models and investigate adaptive mechanisms for automatic model selection. As neural network acceleration hardware becomes more prevalent, the computational gap between traditional and neural models may narrow, potentially shifting the balance of trade-offs documented in this study.
