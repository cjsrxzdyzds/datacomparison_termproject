\section{Conclusion}
\label{sec:conclusion}

This paper presented a comprehensive comparative analysis of probability models for arithmetic coding, evaluating Markov models, Finite State Machine models, and neural network models across diverse datasets.

\subsection{Key Findings}
Our experimental results demonstrate:
\begin{enumerate}
    \item \textbf{Higher-order models excel on text}: The 3rd Order Markov model achieved the best compression ratio on English text, validating the importance of context.
    \item \textbf{RNNs show promise}: The simple RNN implementation was competitive with Markov models and outperformed the FSM on text, suggesting that neural approaches can capture complex dependencies.
    \item \textbf{Precision matters for binary data}: We identified that standard floating-point arithmetic coding can struggle with the high-entropy nature of binary data, requiring higher precision (52-bit) to avoid decoding errors.
    \item \textbf{Trade-offs are unavoidable}: The best compressing models were also the slowest, confirming the fundamental trade-off between compression efficiency and computational complexity.
\end{enumerate}

\subsection{Practical Contributions}
This work provides:
\begin{itemize}
    \item A unified framework for comparing diverse probability models
    \item Quantitative guidelines for model selection
    \item Open-source MATLAB implementation for educational use
    \item Empirical evidence of compression-complexity trade-offs
\end{itemize}

\subsection{Broader Impact}
Understanding the trade-offs between probability models for arithmetic coding has implications for:
\begin{itemize}
    \item Designing efficient compression systems for resource-constrained devices
    \item Developing adaptive compressors that select models based on data characteristics
    \item Advancing neural compression techniques by identifying their strengths and limitations
\end{itemize}

\subsection{Final Remarks}
While arithmetic coding with sophisticated probability models can approach theoretical entropy limits, practical considerations of speed and memory often favor simpler models. The optimal choice depends on the specific application context, and our comparative analysis provides the empirical foundation for making informed decisions.

Future work should explore hybrid approaches that combine the strengths of different models and investigate adaptive mechanisms for automatic model selection. As neural network acceleration hardware becomes more prevalent, the computational gap between traditional and neural models may narrow, potentially shifting the balance of trade-offs documented in this study.
