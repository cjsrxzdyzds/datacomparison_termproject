\section{Experimental Setup}
\label{sec:experimental_setup}

\subsection{Datasets}
We evaluate all models on three categories of data to assess performance across different characteristics:

\subsubsection{Text Data}
\begin{itemize}
    \item \textbf{alice29.txt}: English text from "Alice in Wonderland" (152 KB)
    \item \textbf{asyoulik.txt}: Shakespeare play "As You Like It" (125 KB)
    \item \textbf{lcet10.txt}: Technical writing (426 KB)
    \item \textbf{plrabn12.txt}: Poetry from "Paradise Lost" (481 KB)
\end{itemize}

\subsubsection{Binary and Source Code}
\begin{itemize}
    \item \textbf{kennedy.xls}: Excel spreadsheet (1 MB)
    \item \textbf{cp.html}: HTML source code (24 KB)
    \item \textbf{fields.c}: C source code (11 KB)
    \item \textbf{grammar.lsp}: LISP source code (3 KB)
    \item \textbf{xargs.1}: Man page (4 KB)
\end{itemize}

\subsubsection{Sequence Data}
\begin{itemize}
    \item \textbf{sum}: SPARC executable (38 KB)
    \item \textbf{ptt5}: Fax image (CCITT 1D) (513 KB)
\end{itemize}

\subsection{Evaluation Metrics}

\subsubsection{Compression Performance}
\begin{itemize}
    \item \textbf{Compression ratio}: $\frac{\text{compressed size}}{\text{original size}}$
    \item \textbf{Bits per symbol}: $\frac{\text{compressed size in bits}}{\text{number of symbols}}$
    \item \textbf{Compression gain}: Comparison against zero-order entropy and Huffman coding
\end{itemize}

\subsubsection{Computational Cost}
\begin{itemize}
    \item \textbf{Encoding time}: Wall-clock time to compress data
    \item \textbf{Decoding time}: Wall-clock time to decompress data
    \item \textbf{Memory usage}: Peak RAM consumption during encoding
\end{itemize}

\subsubsection{Baseline Comparisons}
We compare against:
\begin{itemize}
    \item Zero-order entropy (theoretical lower bound assuming i.i.d. symbols)
    \item Huffman coding with adaptive frequencies
    \item gzip (DEFLATE algorithm)
    \item bzip2 (Burrows-Wheeler transform with Huffman coding)
\end{itemize}

\subsection{Experimental Procedure}
For each dataset and model combination:
\begin{enumerate}
    \item Initialize the model with uniform probabilities
    \item Encode the file symbol-by-symbol, updating model adaptively
    \item Record compressed size and encoding time
    \item Decode the compressed file to verify correctness
    \item Record decoding time and peak memory usage
    \item Repeat for 3 trials and report mean and standard deviation
\end{enumerate}

\subsection{Hardware and Software}
\begin{itemize}
    \item \textbf{Processor}: Apple M-Series (ARM64)
    \item \textbf{RAM}: 16 GB Unified Memory
    \item \textbf{Operating System}: macOS Sequoia 15.1
    \item \textbf{Software}: MATLAB R2025a
\end{itemize}
