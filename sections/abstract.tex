\begin{abstract}
This paper presents a comparative analysis of probability models for arithmetic coding in data compression. We implement and evaluate three classes of models: Markov models (1st, 2nd, and 3rd order), Finite State Machine (FSM) models, and neural network models (RNN/LSTM). Each model is integrated with an arithmetic coder and tested on diverse datasets including text files, binary data, and DNA sequences. Our evaluation focuses on compression ratio, encoding/decoding time, and memory usage. The results provide practical guidelines for selecting appropriate probability models based on data characteristics and application requirements, demonstrating the trade-offs between model complexity and compression performance.
\end{abstract}
