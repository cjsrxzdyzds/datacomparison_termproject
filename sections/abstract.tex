\begin{abstract}
This paper presents a comparative analysis of probability models for arithmetic coding in data compression. We implement and evaluate three classes of models: Markov models (1st, 2nd, and 3rd order), Finite State Machine (FSM) models, and a Simple Recurrent Neural Network (RNN). Each model is integrated with an arithmetic coder and tested on diverse datasets including English text, binary executables, and DNA sequences. Our evaluation focuses on compression ratio (bits per symbol), encoding/decoding time, and memory usage. The results demonstrate that while the 3rd Order Markov model achieves the best compression on text (4.76 bps), it incurs a significant computational cost. The RNN model offers a competitive alternative, outperforming lower-order Markov models with potentially better scalability. We also highlight the challenges of applying these models to binary data, where floating-point precision can lead to decoding errors.
\end{abstract}
