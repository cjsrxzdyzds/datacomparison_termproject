\begin{abstract}
This paper presents a comparative analysis of probability models for arithmetic coding in data compression. We implement and evaluate three classes of models: Markov models (1st, 2nd, and 3rd order), Finite State Machine (FSM) models, and a Long Short-Term Memory (LSTM) network. Each model is integrated with an arithmetic coder and tested on diverse datasets including English text, binary executables, and DNA sequences. Our evaluation focuses on compression ratio (bits per symbol), encoding/decoding time, and memory usage. The results demonstrate that the LSTM model achieves the best compression on text (avg. 3.26 bps) and binary data, effectively capturing complex dependencies where statistical models struggle. However, this comes at a significant computational cost. The 3rd Order Markov model, while theoretically powerful, suffered from context dilution on smaller datasets. We also highlight the challenges of applying these models to high-entropy binary data.
\end{abstract}
