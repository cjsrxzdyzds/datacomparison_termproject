\subsection{Phase 2 Verification}
We extended our verification to the advanced probability models implemented in Phase 2: 3rd Order Markov, Finite State Machine (FSM), and Long Short-Term Memory (LSTM) network.

\subsubsection{High-Order Markov and Neural Models}
We tested the 3rd Order Markov and LSTM models on the same test string as Phase 1.

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Original Bits} & \textbf{Compressed Bits} & \textbf{Bits Per Symbol} \\
\midrule
3rd Order Markov & 456 & 457 & 8.02 \\
LSTM (Neural) & 456 & 451 & 7.91 \\
\bottomrule
\end{tabular}
\caption{Phase 2 verification results on a 57-byte test string.}
\label{tab:phase2_verification}
\end{table}

The LSTM model achieved a compression ratio of 7.91 bps on this short string. While slightly higher than the previous simple RNN (7.77 bps) due to the increased parameter count (gates) requiring more data to adapt, the LSTM architecture provides significantly better capacity for modeling long-term dependencies in larger files. The 3rd Order Markov model performed similarly (8.02 bps), highlighting the difficulty of compressing very short strings with complex adaptive models.

\subsubsection{FSM Model (Run-Length)}
To verify the FSM model's ability to handle run-length sequences, we tested it on a synthetic string containing repeated characters ("AAAAABBBBB...").

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Original Bits} & \textbf{Compressed Bits} & \textbf{Bits Per Symbol} \\
\midrule
FSM Model & 240 & 226 & 7.53 \\
\bottomrule
\end{tabular}
\caption{FSM verification on a run-length sequence (30 symbols).}
\label{tab:fsm_verification}
\end{table}

The FSM model successfully compressed the run-length sequence to 7.53 bits/symbol, confirming its effectiveness in detecting and exploiting repeated patterns.
