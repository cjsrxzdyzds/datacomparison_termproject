\subsection{Comparison with Baselines}
To contextualize our results, we compared our best-performing models (LSTM and Context Mixing) against standard industry compressors: \texttt{gzip} (DEFLATE) and \texttt{bzip2} (Burrows-Wheeler Transform).

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Compressor} & \textbf{alice29.txt (bps)} & \textbf{kennedy.xls (bps)} \\ \hline
gzip (DEFLATE) & 2.86 & 1.58 \\ \hline
bzip2 (BWT) & 2.27 & 1.01 \\ \hline
\textbf{Ours (LSTM)} & \textbf{2.41} & \textbf{0.92} \\ \hline
Ours (Mixing) & 2.15 & 1.10 \\ \hline
\end{tabular}
\caption{Compression performance (bits per symbol) vs. standard tools. Lower is better.}
\label{tab:baseline}
\end{table}

Our LSTM model outperforms \texttt{gzip} on both text and binary data and achieves superior compression to \texttt{bzip2} on the binary dataset (`kennedy.xls`). This highlights the power of neural probability models in capturing complex, non-linear dependencies that traditional dictionary or block-sorting methods may miss. However, it is important to note that our implementation is significantly slower than these optimized C tools.
