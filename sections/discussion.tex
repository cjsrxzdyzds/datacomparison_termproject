\section{Discussion}
\label{sec:discussion}

\subsection{Compression-Complexity Trade-off}
% Analyze the fundamental trade-off
Our results reveal a clear trade-off between compression performance and computational complexity. The 3rd Order Markov model achieved the highest compression ratios on text data but required approximately 10 times more encoding time than the 1st Order model. Conversely, the FSM model offered a balanced approach for structured data, providing moderate compression gains with minimal computational overhead. The RNN model, while promising in terms of compression potential (especially for complex dependencies), incurred the highest computational cost, making it less suitable for real-time applications without hardware acceleration.

\subsubsection{Model Complexity vs. Compression Gain}
Higher-order models generally achieve better compression, but with diminishing returns. For instance, moving from a 1st Order to a 2nd Order Markov model on the `alice29.txt` dataset yielded a 5\% improvement in compression ratio, but doubling the order to 3rd Order only provided an additional 2\% gain while increasing memory usage by a factor of 256 (due to the $256^3$ state space). This suggests that for many practical applications, lower-order models or FSMs may offer a more optimal balance.

\subsubsection{Practical Implications}
For applications with:
\begin{itemize}
    \item \textbf{Limited CPU}: First-order Markov or FSM models provide good balance
    \item \textbf{High compression priority}: Third-order Markov or neural models
    \item \textbf{Real-time requirements}: FSM models with specialized state machines
\end{itemize}

\subsection{Data Type Characteristics}

\subsubsection{Text vs. Binary Data}
Text data shows strong local correlations that are well-captured by Markov models, as evidenced by the superior performance of the 3rd Order model on the Canterbury Corpus text files. Binary data, however, exhibits higher entropy and less predictable byte-level patterns. In these cases, the FSM model's ability to detect specific structural repetitions (like run-lengths) proved more effective than pure statistical modeling. This suggests that domain-specific knowledge (e.g., file format structure) is crucial for compressing binary executables effectively.

DNA and protein sequences benefit significantly from higher-order Markov models due to the biological constraints that govern sequence formation (e.g., codon triplets). Our experiments showed that the 3rd Order Markov model outperformed others on the `sum` and `ptt5` datasets, likely capturing these inherent structural dependencies.

\subsubsection{Implications of Model Correlation}
The correlation analysis (Figure~\ref{fig:correlation}) provides a theoretical basis for our Context Mixing results. Since the RNN and Markov models are relatively uncorrelated, they are ideal candidates for mixing; their combined prediction tends to be more robust than either alone. Conversely, mixing highly correlated models (like Markov 1 and FSM on random data) offers diminishing returns, as they tend to make the same errors.

\subsection{Model Selection Guidelines}
Based on our experiments, we recommend:

\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{Data Type} & \textbf{Priority} & \textbf{Recommended Model} \\ \hline
Text & Speed & 1st order Markov \\ \hline
Text & Compression & LSTM \\ \hline
Binary & Balanced & FSM \\ \hline
Binary & Compression & LSTM \\ \hline
DNA & Compression & 2nd order Markov \\ \hline
General & Adaptive & LSTM (if resources allow) \\ \hline
\end{tabular}
\caption{Model selection guidelines based on data type and priority.}
\label{tab:guidelines}
\end{table}

\subsection{Limitations}

\subsubsection{Implementation Constraints}
Our MATLAB implementation may not reflect optimized C/C++ performance. The overhead of the MATLAB interpreter, particularly for object-oriented handle classes and frequent function calls in the arithmetic coding loop, significantly inflates execution time. Use of the Profiler revealed that dynamic dispatch and memory allocation for temporary arrays were major bottlenecks. While valid for algorithmic comparison, absolute timing results should not be directly compared against production-grade compressors like \texttt{gzip}, which benefit from low-level compiler optimizations and pointer arithmetic.

\subsubsection{Dataset Coverage}
While we tested diverse data types, the scope was limited to the Canterbury Corpus and small custom datasets. The relatively small file sizes (< 1MB) penalize adaptive models, especially high-order Markov and Neural models, which require a "warm-up" period to build accurate probability tables or converge gradients. Initial processing of the file header and early symbols often occurs with a uniform distribution, diluting the overall compression ratio. Evaluation on the Large Corpus or gigabyte-scale datasets would provide clearer insights into the asymptotic behavior of these models.

\subsubsection{Neural Model Training}
Online training of neural models during compression requires updating weights after every symbol, which is computationally expensive. To make this feasible in MATLAB without hardware acceleration, we truncated Backpropagation Through Time (BPTT) to a single step. While this limits the model's ability to learn long-term dependencies via the gradient signal, the LSTM's cell state still carries historical information forward. This design choice strikes a balance between execution speed and the ability to capture local statistical patterns, though a full BPTT implementation would likely yield higher compression ratios at the cost of significantly increased encoding time.

\subsection{Future Work}
Promising directions for future research include:
\begin{itemize}
    \item Hybrid models combining Markov and neural approaches
    \item Context mixing techniques to leverage multiple models
    \item Hardware acceleration for neural probability models
    \item Adaptive model selection that switches based on data characteristics
\end{itemize}
